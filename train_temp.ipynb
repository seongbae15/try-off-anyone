{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba9139b",
   "metadata": {},
   "source": [
    "# Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a814253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from accelerate.logging import get_logger\n",
    "from diffusers import DDIMScheduler, AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils.torch_utils import randn_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77d3ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_path\",\n",
    "        type=str,\n",
    "        default=\"stable-diffusion-v1-5/stable-diffusion-inpainting\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--vae_model_path\", type=str, default=\"stabilityai/sd-vae-ft-mse\"\n",
    "    )\n",
    "    parser.add_argument(\"--dataset_path\", type=str, default=\"./data/zalando-hd-resized\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./ckpt/train-result\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--resolution_height\", type=int, default=512)\n",
    "    parser.add_argument(\"--resolution_width\", type=int, default=384)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=4)\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=20)\n",
    "    parser.add_argument(\"--max_train_steps\", type=int, default=None)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--lr_scheduler\", type=str, default=\"cosine\")\n",
    "    parser.add_argument(\"--lr_warmup_steps\", type=int, default=0)\n",
    "    parser.add_argument(\"--adam_beta1\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--adam_beta2\", type=float, default=0.999)\n",
    "    parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2)\n",
    "    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08)\n",
    "    parser.add_argument(\"--logging_dir\", type=str, default=\"logs\")\n",
    "    parser.add_argument(\"--mixed_precision\", type=str, default=\"no\")\n",
    "    parser.add_argument(\"--checkpointing_steps\", type=int, default=1000)\n",
    "    parser.add_argument(\"--checkpoints_total_limit\", type=int, default=1)\n",
    "    parser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00745f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502d74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp args for test\n",
    "args.num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d4c068",
   "metadata": {},
   "source": [
    "# Set Logger and Accelerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1887a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "logging_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1a74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    log_with=\"tensorboard\",\n",
    "    project_dir=logging_dir,\n",
    ")\n",
    "\n",
    "logger = get_logger(\"mac-train-1epoch\")\n",
    "\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d92d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.init_trackers(\"mac-train-1epoch\", config=vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b43beb",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46af86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VTONDataset(Dataset):\n",
    "    def __init__(self, dataset_path, split=\"train\", height=512, width=384):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        # set directory path\n",
    "        self.image_person_dir = Path(self.dataset_path).joinpath(split, \"image\")\n",
    "        self.image_cloth_dir = Path(self.dataset_path).joinpath(split, \"cloth\")\n",
    "        self.mask_cloth_person_dir = Path(self.dataset_path).joinpath(\n",
    "            split, \"agnostic-mask\"\n",
    "        )\n",
    "\n",
    "        # Check if directories exist\n",
    "        if not self.image_person_dir.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Image person directory not found: {self.image_person_dir}\"\n",
    "            )\n",
    "        if not self.image_cloth_dir.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Image cloth directory not found: {self.image_cloth_dir}\"\n",
    "            )\n",
    "        if not self.mask_cloth_person_dir.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Mask cloth person directory not found: {self.mask_cloth_person_dir}\"\n",
    "            )\n",
    "\n",
    "        self.image_persion_files = sorted(self.image_person_dir.glob(\"*.jpg\"))\n",
    "\n",
    "        self.valid_files = []\n",
    "        for person_img_path in self.image_persion_files:\n",
    "            base_name = person_img_path.stem\n",
    "            cloth_img_path = Path(self.image_cloth_dir).joinpath(f\"{base_name}.jpg\")\n",
    "            mask_img_path = Path(self.mask_cloth_person_dir).joinpath(\n",
    "                f\"{base_name}_mask.png\"\n",
    "            )\n",
    "\n",
    "            if cloth_img_path.exists() and mask_img_path.exists():\n",
    "                self.valid_files.append(\n",
    "                    {\n",
    "                        \"person\": person_img_path,\n",
    "                        \"cloth\": cloth_img_path,\n",
    "                        \"mask\": mask_img_path,\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                logger.warning(f\"Skip {base_name}\")\n",
    "\n",
    "        if not self.valid_files:\n",
    "            raise ValueError(f\"No valid files found in {self.split} split.\")\n",
    "\n",
    "        # Image preprocessor\n",
    "        self.vae_image_processor = VaeImageProcessor(\n",
    "            vae_scale_factor=8, do_normalize=True, do_convert_rgb=True\n",
    "        )\n",
    "        # Mask preprocessor\n",
    "        self.mask_processor = VaeImageProcessor(\n",
    "            vae_scale_factor=8,\n",
    "            do_normalize=False,\n",
    "            do_binarize=True,\n",
    "            do_convert_grayscale=True,\n",
    "        )\n",
    "\n",
    "        self.transform_resize = transforms.Resize((self.height, self.width))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.valid_files[idx]\n",
    "\n",
    "        try:\n",
    "            # 이미지 로드\n",
    "            person_image_hc = Image.open(item[\"person\"]).convert(\"RGB\")\n",
    "            cloth_image_c = Image.open(item[\"cloth\"]).convert(\"RGB\")\n",
    "            garment_mask_m = Image.open(item[\"mask\"]).convert(\"L\")\n",
    "\n",
    "            # 리사이즈\n",
    "            person_image_hc = self.transform_resize(person_image_hc)\n",
    "            cloth_image_c = self.transform_resize(cloth_image_c)\n",
    "            garment_mask_m = self.transform_resize(garment_mask_m)\n",
    "\n",
    "            # VAE 프로세서를 사용하여 각 이미지를 전처리합니다.\n",
    "            person_hc_processed = self.vae_image_processor.preprocess(\n",
    "                person_image_hc, self.height, self.width\n",
    "            )[0]\n",
    "            cloth_c_processed = self.vae_image_processor.preprocess(\n",
    "                cloth_image_c, self.height, self.width\n",
    "            )[0]\n",
    "            mask_m_processed = self.mask_processor.preprocess(\n",
    "                garment_mask_m, self.height, self.width\n",
    "            )[0]\n",
    "            mask = self.prepare_mask_image(mask_m_processed)\n",
    "\n",
    "            person_hm_processed = person_hc_processed * (mask < 0.5)\n",
    "\n",
    "            # squeeze를 통해 불필요한 차원 제거\n",
    "            return {\n",
    "                \"person_hc\": person_hc_processed,  # (HC)\n",
    "                \"person_hm\": person_hm_processed,  # (HM) - 새로 추가됨\n",
    "                \"cloth_c\": cloth_c_processed,  # (C)\n",
    "                \"mask_m\": mask_m_processed,  # (M)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"Error processing item at index {idx}: ({item['person'].name}){e}\"\n",
    "            )\n",
    "\n",
    "            if idx > 0:\n",
    "                return self.__getitem__(idx - 1)\n",
    "            else:\n",
    "                dummy_hc = torch.zeros(\n",
    "                    (3, self.height, self.width), dtype=torch.float32\n",
    "                )\n",
    "                dummy_c = torch.zeros((3, self.height, self.width), dtype=torch.float32)\n",
    "                dummy_m = torch.zeros((1, self.height, self.width), dtype=torch.float32)\n",
    "                print(\"It's dummy\")\n",
    "                return {\"person_hc\": dummy_hc, \"cloth_c\": dummy_c, \"mask_m\": dummy_m}\n",
    "\n",
    "    def prepare_mask_image(self, mask_image):\n",
    "        mask_image[mask_image < 0.5] = 0\n",
    "        mask_image[mask_image >= 0.5] = 1\n",
    "        return mask_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b24ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VTONDataset(\n",
    "    args.dataset_path,\n",
    "    split=\"train\",\n",
    "    height=args.resolution_height,\n",
    "    width=args.resolution_width,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=args.train_batch_size,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f304eb",
   "metadata": {},
   "source": [
    "# 모델 및 노이즈 스케줄러 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786180cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skip(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "    ):\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efac0a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuned_modules(unet):\n",
    "    trainable_modules = torch.nn.ModuleList()\n",
    "    for blocks in [unet.down_blocks, unet.mid_block, unet.up_blocks]:\n",
    "        if hasattr(blocks, \"attentions\"):\n",
    "            trainable_modules.append(blocks.attentions)\n",
    "        else:\n",
    "            for block in blocks:\n",
    "                if hasattr(block, \"attentions\"):\n",
    "                    trainable_modules.append(block.attentions)\n",
    "    return trainable_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e8cf57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_cross_attentions(unet):\n",
    "    attn_processors = {\n",
    "        name: unet.attn_processors[name] if name.endswith(\"attn1.processor\") else Skip()\n",
    "        for name in unet.attn_processors.keys()\n",
    "    }\n",
    "    return attn_processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0b00737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Scheduler\n",
    "noise_scheduler = DDIMScheduler.from_pretrained(\n",
    "    args.pretrained_path, subfolder=\"scheduler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be4370c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (up_blocks): ModuleList(\n",
       "      (0-1): 2 x UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VAE\n",
    "vae = AutoencoderKL.from_pretrained(args.vae_model_path)\n",
    "\n",
    "# Freeze VAE\n",
    "vae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e4cccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import load_checkpoint_in_model\n",
    "from accelerate.utils import load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39067fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(args.pretrained_path, subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29e58016",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_trainable = fine_tuned_modules(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9611cd50",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModuleList' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43munet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdown_blocks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/try-off-anyone/lib/python3.11/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'ModuleList' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "unet.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a502995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.0.norm.bias',\n",
       " '0.0.norm.weight',\n",
       " '0.0.proj_in.bias',\n",
       " '0.0.proj_in.weight',\n",
       " '0.0.proj_out.bias',\n",
       " '0.0.proj_out.weight',\n",
       " '0.0.transformer_blocks.0.attn1.to_k.weight',\n",
       " '0.0.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '0.0.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '0.0.transformer_blocks.0.attn1.to_q.weight',\n",
       " '0.0.transformer_blocks.0.attn1.to_v.weight',\n",
       " '0.0.transformer_blocks.0.attn2.to_k.weight',\n",
       " '0.0.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '0.0.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '0.0.transformer_blocks.0.attn2.to_q.weight',\n",
       " '0.0.transformer_blocks.0.attn2.to_v.weight',\n",
       " '0.0.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '0.0.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '0.0.transformer_blocks.0.ff.net.2.bias',\n",
       " '0.0.transformer_blocks.0.ff.net.2.weight',\n",
       " '0.0.transformer_blocks.0.norm1.bias',\n",
       " '0.0.transformer_blocks.0.norm1.weight',\n",
       " '0.0.transformer_blocks.0.norm2.bias',\n",
       " '0.0.transformer_blocks.0.norm2.weight',\n",
       " '0.0.transformer_blocks.0.norm3.bias',\n",
       " '0.0.transformer_blocks.0.norm3.weight',\n",
       " '0.1.norm.bias',\n",
       " '0.1.norm.weight',\n",
       " '0.1.proj_in.bias',\n",
       " '0.1.proj_in.weight',\n",
       " '0.1.proj_out.bias',\n",
       " '0.1.proj_out.weight',\n",
       " '0.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " '0.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '0.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '0.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " '0.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " '0.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " '0.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '0.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '0.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " '0.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " '0.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '0.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '0.1.transformer_blocks.0.ff.net.2.bias',\n",
       " '0.1.transformer_blocks.0.ff.net.2.weight',\n",
       " '0.1.transformer_blocks.0.norm1.bias',\n",
       " '0.1.transformer_blocks.0.norm1.weight',\n",
       " '0.1.transformer_blocks.0.norm2.bias',\n",
       " '0.1.transformer_blocks.0.norm2.weight',\n",
       " '0.1.transformer_blocks.0.norm3.bias',\n",
       " '0.1.transformer_blocks.0.norm3.weight',\n",
       " '1.0.norm.bias',\n",
       " '1.0.norm.weight',\n",
       " '1.0.proj_in.bias',\n",
       " '1.0.proj_in.weight',\n",
       " '1.0.proj_out.bias',\n",
       " '1.0.proj_out.weight',\n",
       " '1.0.transformer_blocks.0.attn1.to_k.weight',\n",
       " '1.0.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '1.0.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '1.0.transformer_blocks.0.attn1.to_q.weight',\n",
       " '1.0.transformer_blocks.0.attn1.to_v.weight',\n",
       " '1.0.transformer_blocks.0.attn2.to_k.weight',\n",
       " '1.0.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '1.0.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '1.0.transformer_blocks.0.attn2.to_q.weight',\n",
       " '1.0.transformer_blocks.0.attn2.to_v.weight',\n",
       " '1.0.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '1.0.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '1.0.transformer_blocks.0.ff.net.2.bias',\n",
       " '1.0.transformer_blocks.0.ff.net.2.weight',\n",
       " '1.0.transformer_blocks.0.norm1.bias',\n",
       " '1.0.transformer_blocks.0.norm1.weight',\n",
       " '1.0.transformer_blocks.0.norm2.bias',\n",
       " '1.0.transformer_blocks.0.norm2.weight',\n",
       " '1.0.transformer_blocks.0.norm3.bias',\n",
       " '1.0.transformer_blocks.0.norm3.weight',\n",
       " '1.1.norm.bias',\n",
       " '1.1.norm.weight',\n",
       " '1.1.proj_in.bias',\n",
       " '1.1.proj_in.weight',\n",
       " '1.1.proj_out.bias',\n",
       " '1.1.proj_out.weight',\n",
       " '1.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " '1.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '1.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '1.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " '1.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " '1.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " '1.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '1.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '1.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " '1.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " '1.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '1.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '1.1.transformer_blocks.0.ff.net.2.bias',\n",
       " '1.1.transformer_blocks.0.ff.net.2.weight',\n",
       " '1.1.transformer_blocks.0.norm1.bias',\n",
       " '1.1.transformer_blocks.0.norm1.weight',\n",
       " '1.1.transformer_blocks.0.norm2.bias',\n",
       " '1.1.transformer_blocks.0.norm2.weight',\n",
       " '1.1.transformer_blocks.0.norm3.bias',\n",
       " '1.1.transformer_blocks.0.norm3.weight',\n",
       " '2.0.norm.bias',\n",
       " '2.0.norm.weight',\n",
       " '2.0.proj_in.bias',\n",
       " '2.0.proj_in.weight',\n",
       " '2.0.proj_out.bias',\n",
       " '2.0.proj_out.weight',\n",
       " '2.0.transformer_blocks.0.attn1.to_k.weight',\n",
       " '2.0.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '2.0.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '2.0.transformer_blocks.0.attn1.to_q.weight',\n",
       " '2.0.transformer_blocks.0.attn1.to_v.weight',\n",
       " '2.0.transformer_blocks.0.attn2.to_k.weight',\n",
       " '2.0.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '2.0.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '2.0.transformer_blocks.0.attn2.to_q.weight',\n",
       " '2.0.transformer_blocks.0.attn2.to_v.weight',\n",
       " '2.0.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '2.0.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '2.0.transformer_blocks.0.ff.net.2.bias',\n",
       " '2.0.transformer_blocks.0.ff.net.2.weight',\n",
       " '2.0.transformer_blocks.0.norm1.bias',\n",
       " '2.0.transformer_blocks.0.norm1.weight',\n",
       " '2.0.transformer_blocks.0.norm2.bias',\n",
       " '2.0.transformer_blocks.0.norm2.weight',\n",
       " '2.0.transformer_blocks.0.norm3.bias',\n",
       " '2.0.transformer_blocks.0.norm3.weight',\n",
       " '2.1.norm.bias',\n",
       " '2.1.norm.weight',\n",
       " '2.1.proj_in.bias',\n",
       " '2.1.proj_in.weight',\n",
       " '2.1.proj_out.bias',\n",
       " '2.1.proj_out.weight',\n",
       " '2.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " '2.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '2.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '2.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " '2.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " '2.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " '2.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '2.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '2.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " '2.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " '2.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '2.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '2.1.transformer_blocks.0.ff.net.2.bias',\n",
       " '2.1.transformer_blocks.0.ff.net.2.weight',\n",
       " '2.1.transformer_blocks.0.norm1.bias',\n",
       " '2.1.transformer_blocks.0.norm1.weight',\n",
       " '2.1.transformer_blocks.0.norm2.bias',\n",
       " '2.1.transformer_blocks.0.norm2.weight',\n",
       " '2.1.transformer_blocks.0.norm3.bias',\n",
       " '2.1.transformer_blocks.0.norm3.weight',\n",
       " '3.0.norm.bias',\n",
       " '3.0.norm.weight',\n",
       " '3.0.proj_in.bias',\n",
       " '3.0.proj_in.weight',\n",
       " '3.0.proj_out.bias',\n",
       " '3.0.proj_out.weight',\n",
       " '3.0.transformer_blocks.0.attn1.to_k.weight',\n",
       " '3.0.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '3.0.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '3.0.transformer_blocks.0.attn1.to_q.weight',\n",
       " '3.0.transformer_blocks.0.attn1.to_v.weight',\n",
       " '3.0.transformer_blocks.0.attn2.to_k.weight',\n",
       " '3.0.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '3.0.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '3.0.transformer_blocks.0.attn2.to_q.weight',\n",
       " '3.0.transformer_blocks.0.attn2.to_v.weight',\n",
       " '3.0.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '3.0.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '3.0.transformer_blocks.0.ff.net.2.bias',\n",
       " '3.0.transformer_blocks.0.ff.net.2.weight',\n",
       " '3.0.transformer_blocks.0.norm1.bias',\n",
       " '3.0.transformer_blocks.0.norm1.weight',\n",
       " '3.0.transformer_blocks.0.norm2.bias',\n",
       " '3.0.transformer_blocks.0.norm2.weight',\n",
       " '3.0.transformer_blocks.0.norm3.bias',\n",
       " '3.0.transformer_blocks.0.norm3.weight',\n",
       " '4.0.norm.bias',\n",
       " '4.0.norm.weight',\n",
       " '4.0.proj_in.bias',\n",
       " '4.0.proj_in.weight',\n",
       " '4.0.proj_out.bias',\n",
       " '4.0.proj_out.weight',\n",
       " '4.0.transformer_blocks.0.attn1.to_k.weight',\n",
       " '4.0.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '4.0.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '4.0.transformer_blocks.0.attn1.to_q.weight',\n",
       " '4.0.transformer_blocks.0.attn1.to_v.weight',\n",
       " '4.0.transformer_blocks.0.attn2.to_k.weight',\n",
       " '4.0.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '4.0.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '4.0.transformer_blocks.0.attn2.to_q.weight',\n",
       " '4.0.transformer_blocks.0.attn2.to_v.weight',\n",
       " '4.0.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '4.0.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '4.0.transformer_blocks.0.ff.net.2.bias',\n",
       " '4.0.transformer_blocks.0.ff.net.2.weight',\n",
       " '4.0.transformer_blocks.0.norm1.bias',\n",
       " '4.0.transformer_blocks.0.norm1.weight',\n",
       " '4.0.transformer_blocks.0.norm2.bias',\n",
       " '4.0.transformer_blocks.0.norm2.weight',\n",
       " '4.0.transformer_blocks.0.norm3.bias',\n",
       " '4.0.transformer_blocks.0.norm3.weight',\n",
       " '4.1.norm.bias',\n",
       " '4.1.norm.weight',\n",
       " '4.1.proj_in.bias',\n",
       " '4.1.proj_in.weight',\n",
       " '4.1.proj_out.bias',\n",
       " '4.1.proj_out.weight',\n",
       " '4.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " '4.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '4.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '4.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " '4.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " '4.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " '4.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '4.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '4.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " '4.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " '4.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '4.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '4.1.transformer_blocks.0.ff.net.2.bias',\n",
       " '4.1.transformer_blocks.0.ff.net.2.weight',\n",
       " '4.1.transformer_blocks.0.norm1.bias',\n",
       " '4.1.transformer_blocks.0.norm1.weight',\n",
       " '4.1.transformer_blocks.0.norm2.bias',\n",
       " '4.1.transformer_blocks.0.norm2.weight',\n",
       " '4.1.transformer_blocks.0.norm3.bias',\n",
       " '4.1.transformer_blocks.0.norm3.weight',\n",
       " '4.2.norm.bias',\n",
       " '4.2.norm.weight',\n",
       " '4.2.proj_in.bias',\n",
       " '4.2.proj_in.weight',\n",
       " '4.2.proj_out.bias',\n",
       " '4.2.proj_out.weight',\n",
       " '4.2.transformer_blocks.0.attn1.to_k.weight',\n",
       " '4.2.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '4.2.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '4.2.transformer_blocks.0.attn1.to_q.weight',\n",
       " '4.2.transformer_blocks.0.attn1.to_v.weight',\n",
       " '4.2.transformer_blocks.0.attn2.to_k.weight',\n",
       " '4.2.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '4.2.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '4.2.transformer_blocks.0.attn2.to_q.weight',\n",
       " '4.2.transformer_blocks.0.attn2.to_v.weight',\n",
       " '4.2.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '4.2.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '4.2.transformer_blocks.0.ff.net.2.bias',\n",
       " '4.2.transformer_blocks.0.ff.net.2.weight',\n",
       " '4.2.transformer_blocks.0.norm1.bias',\n",
       " '4.2.transformer_blocks.0.norm1.weight',\n",
       " '4.2.transformer_blocks.0.norm2.bias',\n",
       " '4.2.transformer_blocks.0.norm2.weight',\n",
       " '4.2.transformer_blocks.0.norm3.bias',\n",
       " '4.2.transformer_blocks.0.norm3.weight',\n",
       " '5.0.norm.bias',\n",
       " '5.0.norm.weight',\n",
       " '5.0.proj_in.bias',\n",
       " '5.0.proj_in.weight',\n",
       " '5.0.proj_out.bias',\n",
       " '5.0.proj_out.weight',\n",
       " '5.0.transformer_blocks.0.attn1.to_k.weight',\n",
       " '5.0.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '5.0.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '5.0.transformer_blocks.0.attn1.to_q.weight',\n",
       " '5.0.transformer_blocks.0.attn1.to_v.weight',\n",
       " '5.0.transformer_blocks.0.attn2.to_k.weight',\n",
       " '5.0.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '5.0.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '5.0.transformer_blocks.0.attn2.to_q.weight',\n",
       " '5.0.transformer_blocks.0.attn2.to_v.weight',\n",
       " '5.0.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '5.0.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '5.0.transformer_blocks.0.ff.net.2.bias',\n",
       " '5.0.transformer_blocks.0.ff.net.2.weight',\n",
       " '5.0.transformer_blocks.0.norm1.bias',\n",
       " '5.0.transformer_blocks.0.norm1.weight',\n",
       " '5.0.transformer_blocks.0.norm2.bias',\n",
       " '5.0.transformer_blocks.0.norm2.weight',\n",
       " '5.0.transformer_blocks.0.norm3.bias',\n",
       " '5.0.transformer_blocks.0.norm3.weight',\n",
       " '5.1.norm.bias',\n",
       " '5.1.norm.weight',\n",
       " '5.1.proj_in.bias',\n",
       " '5.1.proj_in.weight',\n",
       " '5.1.proj_out.bias',\n",
       " '5.1.proj_out.weight',\n",
       " '5.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " '5.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '5.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '5.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " '5.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " '5.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " '5.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '5.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '5.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " '5.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " '5.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '5.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '5.1.transformer_blocks.0.ff.net.2.bias',\n",
       " '5.1.transformer_blocks.0.ff.net.2.weight',\n",
       " '5.1.transformer_blocks.0.norm1.bias',\n",
       " '5.1.transformer_blocks.0.norm1.weight',\n",
       " '5.1.transformer_blocks.0.norm2.bias',\n",
       " '5.1.transformer_blocks.0.norm2.weight',\n",
       " '5.1.transformer_blocks.0.norm3.bias',\n",
       " '5.1.transformer_blocks.0.norm3.weight',\n",
       " '5.2.norm.bias',\n",
       " '5.2.norm.weight',\n",
       " '5.2.proj_in.bias',\n",
       " '5.2.proj_in.weight',\n",
       " '5.2.proj_out.bias',\n",
       " '5.2.proj_out.weight',\n",
       " '5.2.transformer_blocks.0.attn1.to_k.weight',\n",
       " '5.2.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '5.2.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '5.2.transformer_blocks.0.attn1.to_q.weight',\n",
       " '5.2.transformer_blocks.0.attn1.to_v.weight',\n",
       " '5.2.transformer_blocks.0.attn2.to_k.weight',\n",
       " '5.2.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '5.2.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '5.2.transformer_blocks.0.attn2.to_q.weight',\n",
       " '5.2.transformer_blocks.0.attn2.to_v.weight',\n",
       " '5.2.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '5.2.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '5.2.transformer_blocks.0.ff.net.2.bias',\n",
       " '5.2.transformer_blocks.0.ff.net.2.weight',\n",
       " '5.2.transformer_blocks.0.norm1.bias',\n",
       " '5.2.transformer_blocks.0.norm1.weight',\n",
       " '5.2.transformer_blocks.0.norm2.bias',\n",
       " '5.2.transformer_blocks.0.norm2.weight',\n",
       " '5.2.transformer_blocks.0.norm3.bias',\n",
       " '5.2.transformer_blocks.0.norm3.weight',\n",
       " '6.0.norm.bias',\n",
       " '6.0.norm.weight',\n",
       " '6.0.proj_in.bias',\n",
       " '6.0.proj_in.weight',\n",
       " '6.0.proj_out.bias',\n",
       " '6.0.proj_out.weight',\n",
       " '6.0.transformer_blocks.0.attn1.to_k.weight',\n",
       " '6.0.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '6.0.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '6.0.transformer_blocks.0.attn1.to_q.weight',\n",
       " '6.0.transformer_blocks.0.attn1.to_v.weight',\n",
       " '6.0.transformer_blocks.0.attn2.to_k.weight',\n",
       " '6.0.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '6.0.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '6.0.transformer_blocks.0.attn2.to_q.weight',\n",
       " '6.0.transformer_blocks.0.attn2.to_v.weight',\n",
       " '6.0.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '6.0.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '6.0.transformer_blocks.0.ff.net.2.bias',\n",
       " '6.0.transformer_blocks.0.ff.net.2.weight',\n",
       " '6.0.transformer_blocks.0.norm1.bias',\n",
       " '6.0.transformer_blocks.0.norm1.weight',\n",
       " '6.0.transformer_blocks.0.norm2.bias',\n",
       " '6.0.transformer_blocks.0.norm2.weight',\n",
       " '6.0.transformer_blocks.0.norm3.bias',\n",
       " '6.0.transformer_blocks.0.norm3.weight',\n",
       " '6.1.norm.bias',\n",
       " '6.1.norm.weight',\n",
       " '6.1.proj_in.bias',\n",
       " '6.1.proj_in.weight',\n",
       " '6.1.proj_out.bias',\n",
       " '6.1.proj_out.weight',\n",
       " '6.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " '6.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '6.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '6.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " '6.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " '6.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " '6.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '6.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '6.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " '6.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " '6.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '6.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '6.1.transformer_blocks.0.ff.net.2.bias',\n",
       " '6.1.transformer_blocks.0.ff.net.2.weight',\n",
       " '6.1.transformer_blocks.0.norm1.bias',\n",
       " '6.1.transformer_blocks.0.norm1.weight',\n",
       " '6.1.transformer_blocks.0.norm2.bias',\n",
       " '6.1.transformer_blocks.0.norm2.weight',\n",
       " '6.1.transformer_blocks.0.norm3.bias',\n",
       " '6.1.transformer_blocks.0.norm3.weight',\n",
       " '6.2.norm.bias',\n",
       " '6.2.norm.weight',\n",
       " '6.2.proj_in.bias',\n",
       " '6.2.proj_in.weight',\n",
       " '6.2.proj_out.bias',\n",
       " '6.2.proj_out.weight',\n",
       " '6.2.transformer_blocks.0.attn1.to_k.weight',\n",
       " '6.2.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " '6.2.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " '6.2.transformer_blocks.0.attn1.to_q.weight',\n",
       " '6.2.transformer_blocks.0.attn1.to_v.weight',\n",
       " '6.2.transformer_blocks.0.attn2.to_k.weight',\n",
       " '6.2.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " '6.2.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " '6.2.transformer_blocks.0.attn2.to_q.weight',\n",
       " '6.2.transformer_blocks.0.attn2.to_v.weight',\n",
       " '6.2.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " '6.2.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " '6.2.transformer_blocks.0.ff.net.2.bias',\n",
       " '6.2.transformer_blocks.0.ff.net.2.weight',\n",
       " '6.2.transformer_blocks.0.norm1.bias',\n",
       " '6.2.transformer_blocks.0.norm1.weight',\n",
       " '6.2.transformer_blocks.0.norm2.bias',\n",
       " '6.2.transformer_blocks.0.norm2.weight',\n",
       " '6.2.transformer_blocks.0.norm3.bias',\n",
       " '6.2.transformer_blocks.0.norm3.weight'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(unet_trainable.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf93e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_checkpoint = load_state_dict(\n",
    "    \"/Users/seongbae/workspace/ezpz-test/try-off-anyone/ckpt-test2/unet_transformer_block.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e69cab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "resutl = unet.load_state_dict(loaded_checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b25bb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resutl.missing_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "44c9fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_loaded in set(loaded_checkpoint.keys()):\n",
    "    if \"conv_in.weight\" in k_loaded:\n",
    "        print(k_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60df1c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(args.pretrained_path, subfolder=\"unet\")\n",
    "\n",
    "# 확인용: 파라미터 이름 확인\n",
    "all_trainable = set(\n",
    "    name for name, param in unet.named_parameters() if param.requires_grad\n",
    ")\n",
    "\n",
    "# Freeze all U-Net parameters first\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "# Unfreeze only transformer block\n",
    "trainable_unet_module = fine_tuned_modules(unet)\n",
    "if not trainable_unet_module:\n",
    "    logger.warning(\"No trainable modules identified by fine_tuned_modules.\")\n",
    "    unet.requires_grad_(True)\n",
    "else:\n",
    "    for module_list in trainable_unet_module:\n",
    "        for module in module_list:\n",
    "            if module is not None:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = True\n",
    "    params_to_optimize = []\n",
    "    for module_list in trainable_unet_module:\n",
    "        for module in module_list:\n",
    "            if module is not None:\n",
    "                params_to_optimize.extend(list(module.parameters()))\n",
    "\n",
    "    if not params_to_optimize:\n",
    "        logger.error(\n",
    "            \"No parameters found to optimize even after attempting to unfreeze transformer blocks.\"\n",
    "        )\n",
    "        unet.requires_grad_(True)\n",
    "        params_to_optimize = unet.parameters()\n",
    "\n",
    "# Skip cross-attentions as per\n",
    "unet.set_attn_processor(skip_cross_attentions(unet))\n",
    "\n",
    "# 예시: module_list가 [module1, module2, ...] 형태라고 가정\n",
    "module_trainable = set(\n",
    "    name for name, param in unet.named_parameters() if param.requires_grad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24ccfa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "공통 파라미터: 416\n",
      "전체에만 있는 파라미터: 270\n",
      "module_list에만 있는 파라미터: 0\n"
     ]
    }
   ],
   "source": [
    "# 전체에서 module_list에 포함된 파라미터만 추출\n",
    "intersection = all_trainable & module_trainable\n",
    "# 전체에서 module_list에 없는 파라미터\n",
    "only_in_all = all_trainable - module_trainable\n",
    "# module_list에만 있는 파라미터 (거의 없음)\n",
    "only_in_module = module_trainable - all_trainable\n",
    "\n",
    "print(\"공통 파라미터:\", len(intersection))\n",
    "print(\"전체에만 있는 파라미터:\", len(only_in_all))\n",
    "print(\"module_list에만 있는 파라미터:\", len(only_in_module))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4bb6ea",
   "metadata": {},
   "source": [
    "# Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36df2350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params_to_optimize,\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c3fa8",
   "metadata": {},
   "source": [
    "# Learning Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6343091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "    num_training_steps=(\n",
    "        args.max_train_steps\n",
    "        if args.max_train_steps\n",
    "        else len(train_dataloader) * args.num_train_epochs\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4c462",
   "metadata": {},
   "source": [
    "# Accelerator 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d23e0d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 11647\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2912\n"
     ]
    }
   ],
   "source": [
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    unet, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "weight_dtype = torch.float16 if accelerator.mixed_precision == \"fp16\" else torch.float32\n",
    "vae.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / args.gradient_accumulation_steps\n",
    ")\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "else:\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "total_batch_size = (\n",
    "    args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(f\"  Num examples = {len(train_dataset)}\")\n",
    "print(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "print(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "print(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    ")\n",
    "print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "print(f\"  Total optimization steps = {args.max_train_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a697bc",
   "metadata": {},
   "source": [
    "# Resume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "194f7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCkptsQueue:\n",
    "    def __init__(self, maxlen=2):\n",
    "        self.queue = deque(maxlen=maxlen)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def enqueue(self, item):\n",
    "        if len(self.queue) >= self.maxlen:\n",
    "            removed = self.queue.popleft()  # 가장 오래된 요소 제거\n",
    "            print(f\"Removed: {removed}\")\n",
    "        self.queue.append(item)\n",
    "        print(f\"Added: {item}, SaveCkptsQueue: {list(self.queue)}\")\n",
    "        try:\n",
    "            return removed\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_queue(self):\n",
    "        return list(self.queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74e7e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added: None, SaveCkptsQueue: [None]\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "first_epoch = 0\n",
    "\n",
    "args.resume_from_checkpoint = None\n",
    "old_save_path = None\n",
    "\n",
    "save_path_queue = SaveCkptsQueue(maxlen=args.checkpoints_total_limit)\n",
    "save_path_queue.enqueue(args.resume_from_checkpoint)\n",
    "\n",
    "if args.resume_from_checkpoint:\n",
    "    old_save_path = args.resume_from_checkpoint\n",
    "\n",
    "    if args.resume_from_checkpoint != \"latest\":\n",
    "        path = os.path.basename(args.resume_from_checkpoint)\n",
    "    else:\n",
    "        dirs = os.listdir(args.output_dir)\n",
    "        dirs = [d for d in dirs if d.startswith(\"checkpoint-\")]\n",
    "        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "    if path is None:\n",
    "        accelerator.print(\n",
    "            f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "        )\n",
    "        args.resume_from_checkpoint = None\n",
    "\n",
    "    else:\n",
    "        accelerator.print(f\"Resuming from chekcpoint {path}\")\n",
    "        accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "        global_step = int(path.split(\"-\")[1])\n",
    "        resume_global_step = global_step * args.gradient_accumulation_steps\n",
    "        first_epoch = global_step // num_update_steps_per_epoch\n",
    "        resume_step = resume_global_step % (\n",
    "            num_update_steps_per_epoch * args.gradient_accumulation_steps\n",
    "        )\n",
    "\n",
    "    print(f\"global_step: {global_step}\")\n",
    "    print(f\"first_epoch: {first_epoch}\")\n",
    "    print(f\"resume_global_step: {resume_global_step}\")\n",
    "    print(f\"resume_step: {resume_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7816213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(image, vae):\n",
    "    image = (\n",
    "        image.to(memory_format=torch.contiguous_format)\n",
    "        .float()\n",
    "        .to(vae.device, dtype=vae.dtype)\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        return vae.encode(image).latent_dist.sample() * vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "250b71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_d = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82107b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a89c8e8d8104b74b04ed4a66f1ef4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2912 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(\n",
    "    range(global_step, args.max_train_steps),\n",
    "    disable=not accelerator.is_local_main_process,\n",
    ")\n",
    "\n",
    "progress_bar.set_description(\"Steps\")\n",
    "\n",
    "# for epoch in range(args.num_train_epochs):\n",
    "for epoch in range(first_epoch, args.num_train_epochs):\n",
    "    unet.train()\n",
    "    train_loss = 0.0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n",
    "            if step % args.gradient_accumulation_steps == 0:\n",
    "                progress_bar.update(1)\n",
    "            continue\n",
    "\n",
    "        with accelerator.accumulate(unet):\n",
    "            latents_c = encode(batch[\"cloth_c\"], vae)\n",
    "            latents_hc = encode(batch[\"person_hc\"], vae)\n",
    "            latents_hm = encode(batch[\"person_hm\"], vae)\n",
    "\n",
    "            mask_m_resized = F.interpolate(\n",
    "                batch[\"mask_m\"].to(dtype=weight_dtype),\n",
    "                size=latents_c.shape[-2:],\n",
    "                mode=\"nearest\",\n",
    "            )\n",
    "\n",
    "            latents_x = torch.cat([latents_hm, latents_hc], dim=concat_d)\n",
    "            latents_m = torch.cat(\n",
    "                [mask_m_resized, torch.zeros_like(mask_m_resized)], dim=concat_d\n",
    "            )\n",
    "            latents_cm = torch.cat([latents_c, latents_hc], dim=concat_d)\n",
    "\n",
    "            noise = torch.randn_like(latents_cm)\n",
    "            bsz = latents_cm.shape[0]\n",
    "\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                noise_scheduler.config.num_train_timesteps,\n",
    "                (bsz,),\n",
    "                device=latents_cm.device,\n",
    "            ).long()\n",
    "\n",
    "            # Add noise to the latents_c according to the noise magnitude at each timestep\n",
    "            noisy_latents_cm = noise_scheduler.add_noise(latents_cm, noise, timesteps)\n",
    "\n",
    "            # Prepare U-Net input\n",
    "            model_input = torch.cat([noisy_latents_cm, latents_m, latents_x], dim=1)\n",
    "\n",
    "            # Predict the noise residual\n",
    "            noise_pred = unet(model_input, timesteps, encoder_hidden_states=None).sample\n",
    "\n",
    "            # Calcluate loss\n",
    "            criterion = nn.MSELoss(reduction=\"mean\")\n",
    "            loss = criterion(noise_pred.float(), noise.float())\n",
    "            # loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "\n",
    "            # Gather the losses\n",
    "            # 각 프로세스의 loss만 모아서 평균\n",
    "            losses = accelerator.gather(loss)\n",
    "            avg_loss = losses.mean()\n",
    "            train_loss += avg_loss.item() / args.gradient_accumulation_steps\n",
    "\n",
    "            # Backpropagate\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                params_to_clip = [p for p in params_to_optimize if p.grad is not None]\n",
    "                if params_to_clip:\n",
    "                    accelerator.clip_grad_norm_(params_to_clip, 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "            accelerator.log(\n",
    "                {\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"step_loss\": loss.detach().item(),\n",
    "                    \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                },\n",
    "                step=global_step,\n",
    "            )\n",
    "            train_loss = 0.0\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "        accelerator.save_state(save_path)\n",
    "        logger.info(f\"Saved checkpoint to {save_path}\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Save U-Net\n",
    "        unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "\n",
    "        trainable_state_dict = {}\n",
    "        for name, param in unwrapped_unet.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                trainable_state_dict[name] = param.detach().cpu()\n",
    "\n",
    "        if trainable_state_dict:\n",
    "            torch.save(\n",
    "                trainable_state_dict,\n",
    "                os.path.join(save_path, \"unet_transformer_block.pt\"),\n",
    "            )\n",
    "            shutil.copytree(\n",
    "                os.path.join(save_path),\n",
    "                os.path.join(\n",
    "                    \"/content/drive/MyDrive/01_Project/EZPZ_TryOff_Test/try-off-anyone-ckpts\",\n",
    "                    os.path.basename(save_path),\n",
    "                ),\n",
    "            )\n",
    "            old_save_path = save_path_queue.enqueue(save_path)\n",
    "            time.sleep(5)\n",
    "\n",
    "            if old_save_path:\n",
    "                shutil.rmtree(old_save_path)\n",
    "                shutil.rmtree(\n",
    "                    os.path.join(\n",
    "                        \"/content/drive/MyDrive/01_Project/EZPZ_TryOff_Test/try-off-anyone-ckpts\",\n",
    "                        os.path.basename(old_save_path),\n",
    "                    )\n",
    "                )\n",
    "                time.sleep(5)\n",
    "\n",
    "            logger.info(\n",
    "                f\"Saved fine-tuned U-Net transformer block to {os.path.join(save_path, 'unet_transformer_block.pt')}\"\n",
    "            )\n",
    "        else:\n",
    "            logger.warning(\"No trainable parameters found.\")\n",
    "\n",
    "        logs = {\n",
    "            \"step_loss\": loss.detach().item(),\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        if global_step >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    if global_step >= args.max_train_steps:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final trained_model\n",
    "accelerator.wait_for_everyone()\n",
    "if accelerator.is_main_process:\n",
    "    unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "    # Save only the fine-tuned parameters\n",
    "    final_trainable_state_dict = {}\n",
    "    for name, param in unwrapped_unet.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            final_trainable_state_dict[name] = param.cpu().clone()\n",
    "\n",
    "    if final_trainable_state_dict:\n",
    "        torch.save(\n",
    "            final_trainable_state_dict,\n",
    "            os.path.join(args.output_dir, \"final_unet_transformer_block.pt\"),\n",
    "        )\n",
    "        shutil.copyfile(\n",
    "            os.path.join(args.output_dir, \"final_unet_transformer_block.pt\"),\n",
    "            os.path.join(\n",
    "                \"/content/drive/MyDrive/01_Project/EZPZ_TryOff_Test/try-off-anyone-ckpts\",\n",
    "                \"final_unet_transformer_block.pt\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Saved_final fine-tuned U-Net transformer block to {os.path.join(args.output_dir, 'final_unet_transformer_block.pt')}\"\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"No trainable parameters found.\")\n",
    "        unwrapped_unet.save_pretrained(os.path.join(args.output_dir, \"final_unet_full\"))\n",
    "accelerator.end_training()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0796d7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params_to_optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "02b4204b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.2295, 0.1383, 0.1957, 0.3308, 0.2764, 0.2856, 0.3503, 0.3184, 0.2834,\n",
       "        0.3350, 0.2896, 0.2191, 0.3296, 0.2869, 0.2834, 0.1260, 0.2546, 0.3284,\n",
       "        0.3145, 0.2174, 0.1908, 0.1902, 0.2084, 0.1967, 0.2139, 0.1903, 0.2183,\n",
       "        0.2134, 0.2102, 0.2156, 0.2798, 0.2783, 0.2747, 0.2322, 0.3201, 0.2776,\n",
       "        0.1564, 0.3127, 0.2986, 0.2805, 0.2515, 0.2474, 0.2383, 0.1976, 0.2296,\n",
       "        0.1821, 0.2603, 0.2424, 0.1537, 0.2502, 0.2028, 0.1324, 0.2859, 0.2834,\n",
       "        0.2451, 0.3274, 0.2281, 0.3359, 0.4624, 0.2595, 0.2418, 0.2213, 0.2371,\n",
       "        0.2261, 0.1958, 0.1824, 0.2279, 0.2147, 0.1810, 0.2321, 0.3286, 0.3616,\n",
       "        0.1765, 0.1646, 0.2942, 0.2450, 0.3550, 0.4141, 0.1387, 0.2435, 0.2450,\n",
       "        0.2421, 0.2524, 0.2571, 0.1702, 0.2021, 0.2209, 0.2233, 0.2327, 0.1830,\n",
       "        0.3374, 0.3323, 0.3916, 0.1970, 0.1550, 0.1902, 0.3982, 0.1771, 0.4236,\n",
       "        0.3120, 0.1170, 0.2908, 0.2659, 0.2910, 0.2747, 0.1738, 0.2896, 0.3196,\n",
       "        0.1595, 0.2922, 0.2246, 0.2235, 0.1316, 0.2028, 0.2262, 0.1975, 0.2496,\n",
       "        0.2515, 0.2108, 0.2549, 0.3083, 0.2908, 0.2949, 0.3201, 0.1533, 0.3015,\n",
       "        0.3416, 0.3025, 0.2969, 0.2976, 0.2195, 0.2296, 0.2092, 0.2151, 0.2040,\n",
       "        0.2026, 0.2157, 0.2174, 0.2313, 0.1614, 0.3562, 0.2194, 0.3118, 0.1987,\n",
       "        0.4353, 0.2375, 0.1042, 0.3274, 0.3960, 0.2693, 0.2046, 0.1725, 0.2296,\n",
       "        0.2101, 0.1957, 0.2086, 0.2079, 0.2117, 0.2104, 0.2123, 0.1783, 0.2366,\n",
       "        0.2321, 0.2256, 0.2401, 0.2734, 0.2021, 0.1965, 0.2363, 0.2467, 0.3218,\n",
       "        0.3000, 0.1559, 0.3010, 0.3376, 0.1254, 0.3254, 0.3423, 0.3533, 0.2866,\n",
       "        0.2454, 0.2032, 0.2220, 0.2302, 0.2805, 0.2301, 0.1855, 0.1973, 0.2334,\n",
       "        0.2358, 0.3274, 0.3286, 0.3296, 0.3757, 0.1047, 0.1252, 0.3677, 0.2500,\n",
       "        0.2267, 0.2939, 0.2939, 0.3105, 0.2979, 0.3303, 0.3164, 0.2629, 0.2717,\n",
       "        0.2874, 0.1489, 0.2971, 0.2184, 0.1995, 0.2465, 0.2336, 0.1700, 0.2351,\n",
       "        0.2142, 0.2253, 0.2186, 0.2075, 0.1946, 0.2384, 0.2178, 0.1722, 0.2334,\n",
       "        0.2517, 0.2379, 0.2408, 0.2258, 0.1740, 0.4048, 0.2681, 0.2397, 0.2532,\n",
       "        0.3872, 0.3223, 0.4373, 0.4580, 0.2350, 0.2043, 0.1930, 0.2673, 0.2549,\n",
       "        0.2681, 0.2664, 0.1840, 0.2158, 0.1665, 0.2128, 0.2756, 0.3242, 0.2126,\n",
       "        0.3623, 0.1160, 0.3008, 0.3264, 0.3054, 0.3318, 0.2080, 0.1998, 0.2615,\n",
       "        0.2238, 0.2642, 0.1608, 0.2395, 0.2358, 0.2219, 0.1438, 0.2344, 0.2209,\n",
       "        0.1929, 0.1859, 0.1937, 0.2297, 0.2006, 0.2059, 0.1826, 0.2166, 0.2329,\n",
       "        0.2135, 0.2876, 0.2874, 0.2739, 0.1150, 0.2976, 0.3530, 0.3481, 0.3550,\n",
       "        0.3242, 0.3115, 0.2186, 0.2396, 0.2539, 0.1464, 0.2354, 0.2150, 0.2438,\n",
       "        0.2644, 0.2549, 0.2261, 0.2476, 0.2634, 0.2368, 0.2112, 0.1572, 0.1643,\n",
       "        0.2408, 0.2297, 0.2280, 0.2483, 0.2502, 0.2925, 0.1769, 0.2773, 0.2524,\n",
       "        0.2861, 0.2654, 0.2734, 0.2463, 0.2379], device='mps:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_to_optimize[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584c3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "try-off-anyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
